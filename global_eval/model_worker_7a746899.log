2025-05-20 19:06:36 | INFO | model_worker | args: Namespace(host='localhost', port=21002, worker_address='http://localhost:21002', controller_address='http://localhost:21001', model_path='exported_fedavg_model', revision='main', device='cuda', gpus=None, num_gpus=1, max_gpu_memory=None, dtype=None, load_8bit=False, cpu_offloading=False, gptq_ckpt=None, gptq_wbits=16, gptq_groupsize=-1, gptq_act_order=False, awq_ckpt=None, awq_wbits=16, awq_groupsize=-1, enable_exllama=False, exllama_max_seq_len=4096, exllama_gpu_split=None, exllama_cache_8bit=False, enable_xft=False, xft_max_seq_len=4096, xft_dtype=None, model_names=None, conv_template=None, embed_in_truncate=False, limit_worker_concurrency=5, stream_interval=2, no_register=False, seed=None, debug=False, ssl=False)
2025-05-20 19:06:36 | INFO | model_worker | Loading the model ['exported_fedavg_model'] on worker 7a746899 ...
2025-05-20 19:06:38 | ERROR | stderr | Traceback (most recent call last):
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py", line 1730, in convert_slow_tokenizer
2025-05-20 19:06:38 | ERROR | stderr |     ).converted()
2025-05-20 19:06:38 | ERROR | stderr |       ^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py", line 1624, in converted
2025-05-20 19:06:38 | ERROR | stderr |     tokenizer = self.tokenizer()
2025-05-20 19:06:38 | ERROR | stderr |                 ^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py", line 1617, in tokenizer
2025-05-20 19:06:38 | ERROR | stderr |     vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab_file)
2025-05-20 19:06:38 | ERROR | stderr |                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py", line 1593, in extract_vocab_merges_from_model
2025-05-20 19:06:38 | ERROR | stderr |     bpe_ranks = load_tiktoken_bpe(tiktoken_url)
2025-05-20 19:06:38 | ERROR | stderr |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/tiktoken/load.py", line 148, in load_tiktoken_bpe
2025-05-20 19:06:38 | ERROR | stderr |     contents = read_file_cached(tiktoken_bpe_file, expected_hash)
2025-05-20 19:06:38 | ERROR | stderr |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/tiktoken/load.py", line 48, in read_file_cached
2025-05-20 19:06:38 | ERROR | stderr |     cache_key = hashlib.sha1(blobpath.encode()).hexdigest()
2025-05-20 19:06:38 | ERROR | stderr |                              ^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr | AttributeError: 'NoneType' object has no attribute 'encode'
2025-05-20 19:06:38 | ERROR | stderr | 
2025-05-20 19:06:38 | ERROR | stderr | During handling of the above exception, another exception occurred:
2025-05-20 19:06:38 | ERROR | stderr | 
2025-05-20 19:06:38 | ERROR | stderr | Traceback (most recent call last):
2025-05-20 19:06:38 | ERROR | stderr |   File "<frozen runpy>", line 198, in _run_module_as_main
2025-05-20 19:06:38 | ERROR | stderr |   File "<frozen runpy>", line 88, in _run_code
2025-05-20 19:06:38 | ERROR | stderr |   File "/root/FedAgg/global_eval/FastChat/fastchat/serve/model_worker.py", line 414, in <module>
2025-05-20 19:06:38 | ERROR | stderr |     args, worker = create_model_worker()
2025-05-20 19:06:38 | ERROR | stderr |                    ^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/root/FedAgg/global_eval/FastChat/fastchat/serve/model_worker.py", line 385, in create_model_worker
2025-05-20 19:06:38 | ERROR | stderr |     worker = ModelWorker(
2025-05-20 19:06:38 | ERROR | stderr |              ^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/root/FedAgg/global_eval/FastChat/fastchat/serve/model_worker.py", line 77, in __init__
2025-05-20 19:06:38 | ERROR | stderr |     self.model, self.tokenizer = load_model(
2025-05-20 19:06:38 | ERROR | stderr |                                  ^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/root/FedAgg/global_eval/FastChat/fastchat/model/model_adapter.py", line 373, in load_model
2025-05-20 19:06:38 | ERROR | stderr |     model, tokenizer = adapter.load_model(model_path, kwargs)
2025-05-20 19:06:38 | ERROR | stderr |                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/root/FedAgg/global_eval/FastChat/fastchat/model/model_adapter.py", line 108, in load_model
2025-05-20 19:06:38 | ERROR | stderr |     tokenizer = AutoTokenizer.from_pretrained(
2025-05-20 19:06:38 | ERROR | stderr |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/models/auto/tokenization_auto.py", line 994, in from_pretrained
2025-05-20 19:06:38 | ERROR | stderr |     return tokenizer_class.from_pretrained(
2025-05-20 19:06:38 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py", line 2062, in from_pretrained
2025-05-20 19:06:38 | ERROR | stderr |     return cls._from_pretrained(
2025-05-20 19:06:38 | ERROR | stderr |            ^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py", line 2302, in _from_pretrained
2025-05-20 19:06:38 | ERROR | stderr |     tokenizer = cls(*init_inputs, **init_kwargs)
2025-05-20 19:06:38 | ERROR | stderr |                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_fast.py", line 139, in __init__
2025-05-20 19:06:38 | ERROR | stderr |     fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)
2025-05-20 19:06:38 | ERROR | stderr |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
2025-05-20 19:06:38 | ERROR | stderr |   File "/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py", line 1732, in convert_slow_tokenizer
2025-05-20 19:06:38 | ERROR | stderr |     raise ValueError(
2025-05-20 19:06:38 | ERROR | stderr | ValueError: Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path with a SentencePiece tokenizer.model file.Currently available slow->fast converters: ['AlbertTokenizer', 'BartTokenizer', 'BarthezTokenizer', 'BertTokenizer', 'BigBirdTokenizer', 'BlenderbotTokenizer', 'CamembertTokenizer', 'CLIPTokenizer', 'CodeGenTokenizer', 'ConvBertTokenizer', 'DebertaTokenizer', 'DebertaV2Tokenizer', 'DistilBertTokenizer', 'DPRReaderTokenizer', 'DPRQuestionEncoderTokenizer', 'DPRContextEncoderTokenizer', 'ElectraTokenizer', 'FNetTokenizer', 'FunnelTokenizer', 'GPT2Tokenizer', 'HerbertTokenizer', 'LayoutLMTokenizer', 'LayoutLMv2Tokenizer', 'LayoutLMv3Tokenizer', 'LayoutXLMTokenizer', 'LongformerTokenizer', 'LEDTokenizer', 'LxmertTokenizer', 'MarkupLMTokenizer', 'MBartTokenizer', 'MBart50Tokenizer', 'MPNetTokenizer', 'MobileBertTokenizer', 'MvpTokenizer', 'NllbTokenizer', 'OpenAIGPTTokenizer', 'PegasusTokenizer', 'Qwen2Tokenizer', 'RealmTokenizer', 'ReformerTokenizer', 'RemBertTokenizer', 'RetriBertTokenizer', 'RobertaTokenizer', 'RoFormerTokenizer', 'SeamlessM4TTokenizer', 'SqueezeBertTokenizer', 'T5Tokenizer', 'UdopTokenizer', 'WhisperTokenizer', 'XLMRobertaTokenizer', 'XLNetTokenizer', 'SplinterTokenizer', 'XGLMTokenizer', 'LlamaTokenizer', 'CodeLlamaTokenizer', 'GemmaTokenizer', 'Phi3Tokenizer']
